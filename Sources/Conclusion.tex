\chapter{Conclusion}\label{chap:conclusion}

One of the thesis' goals was to conduct research and evaluate state-of-the-art
deep learning object detection models on real-world datasets provided by the
company SANEZOO EUROPE. We chose the well-known model Faster R-CNN as a
baseline. The next selected model is Cascade R-CNN, which is an extension to the
Faster R-CNN. Both models represent a family of detectors called two-stage
detectors. Finally, we chose a representative of one-stage detectors --
RetinaNet.

At the beginning of the work, we introduced the object detection task's aim and
how to measure the object detection models' performance. We briefly described
the benefits of the deep learning approach compared to the traditional methods.
Afterward, we reviewed the main concepts of deep neural networks. Then, we
described the model architectures with emphasis on understandability by
gradually introducing new building blocks of the detectors.

Next, we introduced the provided datasets. Unfortunately, since the
datasets consist mainly of the company's customer products, only one dataset
could be published. However, we provided illustrations and detailed
characteristics of all datasets. Based on the prior results, we set the hypotheses about
the performance of our models. Afterward, we described the training
process and its details. We discussed the evaluation results dataset by
dataset and eventually summed up the results at the end of \myref{Chapter}{chap:evaluation}.

For the purpose of future evaluations on different datasets, we developed
a simple benchmarking tool with a web GUI. We created a Python API built
upon the scripts we had used for our experiments. The tool provides an
interactive way for training and evaluating arbitrary object detection dataset.

\section{Future Work}
We intentionally chose models that are well-established and have stable
implementations. However, the field of deep learning advances rapidly and we
would like to mention very recent models which we think deserve attention. The
first model, called \bld{EfficientDet} \cite{efficientdet}, was proposed in 2019 by
Google Research. They achieved better results than prior art while reducing the
number of model parameters (weights) and the required computations. Although the
model can be implemented with the standard backbones (e.g., ResNet
\cite{resnet}), better efficiency and performance are achieved using their
proposed backbone \bld{EfficientNet} \cite{efficientnet}.

The next model is a proposal by Facebook AI \cite{detr}. The model achieves similar
performance as highly-optimized Faster R-CNN models on a COCO dataset. The main
contribution of the proposal is the introduction of a new approach to the object
detection task. The model removes hand-designed steps such as non-maximum
suppression or anchor generation (recall Section \ref{rpn}) by incorporating a
transformer encoder-decoder architecture \cite{transformer} that has been
recently exploited in the area of natural language processing (NLP)
\cite{bert, gpt3}. The model is therefore called a \bld{DEtection TRansformer} or
simply \bld{DETR}.

Finally, the last model we would like to mention is the fourth iteration of the
well-known \bld{YOLO} model \cite{yolo1, yolo2, yolo3, yolo4} that achieves similar
performance as EfficientDet while running twice as faster. According to the
authors, the latest extension of YOLOv4 achieves the highest accuracy on the
COCO dataset among any published work \cite{yolo4scaled}.

Our implementation of the benchmarking tool provides rather minimal
functionalities, such as selecting datasets, models, and the training process.
It can be seen as a functional proof-of-concept or a foundation application that
can be further expanded. We list possible extensions below:
\begin{itemize}
      \item Accept more dataset formats such as COCO, VOC, or YOLO dataset format.
      \item Add more models and their settings.
      \item Add more options to change parameters.
      \item Add more control during the training process. The current state does
            not allow any interaction with the training process. It would be
            plausible if the user could pause and resume the training process.
      \item Related to the previous proposal; the user should have clear feedback
            about the training process. The tool should include visualization of
            the losses, e.g., by integrating Tensorboard\footnote{
                  Tensorboard is a visualization kit. See
                  \url{https://tensorboard.dev/}.
            }.
      \item For a long-term goal, it would make sense to separate the evaluation
            component that would serve for evaluating models trained outside the
            tool.
\end{itemize}